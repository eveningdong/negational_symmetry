{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 139048,
     "status": "ok",
     "timestamp": 1594248295381,
     "user": {
      "displayName": "Nanqing Dong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggv6drEa6aTWlDrOv1fiTXBiKpXDvz1Pm8yIE8vgg=s64",
      "userId": "15665172758075970106"
     },
     "user_tz": -60
    },
    "id": "63xKCJZd136n"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_quantum as tfq\n",
    "\n",
    "import cirq\n",
    "import sympy\n",
    "import numpy as np\n",
    "import collections\n",
    "import time\n",
    "\n",
    "# visualization tools\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from cirq.contrib.svg import SVGCircuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 137747,
     "status": "ok",
     "timestamp": 1594248295383,
     "user": {
      "displayName": "Nanqing Dong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggv6drEa6aTWlDrOv1fiTXBiKpXDvz1Pm8yIE8vgg=s64",
      "userId": "15665172758075970106"
     },
     "user_tz": -60
    },
    "id": "oxS5442oNQUX"
   },
   "outputs": [],
   "source": [
    "class CircuitLayerBuilder():\n",
    "    def __init__(self, data_qubits, readout):\n",
    "        self.data_qubits = data_qubits\n",
    "        self.readout = readout\n",
    "    \n",
    "    def add_layer(self, circuit, gate, prefix):\n",
    "        for i, qubit in enumerate(self.data_qubits):\n",
    "            symbol = sympy.Symbol(prefix + '-' + str(i))\n",
    "            circuit.append(gate(qubit, self.readout)**symbol)\n",
    "\n",
    "def create_quantum_model(num_qubits=16):\n",
    "    \"\"\"Create a QNN model circuit and readout operation to go along with it.\"\"\"\n",
    "    h = get_edge(num_qubits)\n",
    "    data_qubits = cirq.GridQubit.rect(h, h)  # a 4x4 grid.\n",
    "    readout = cirq.GridQubit(-1, -1)         # a single qubit at [-1,-1]\n",
    "    circuit = cirq.Circuit()\n",
    "    \n",
    "    # Prepare the readout qubit.\n",
    "    circuit.append(cirq.X(readout))\n",
    "    circuit.append(cirq.H(readout))\n",
    "    \n",
    "    builder = CircuitLayerBuilder(\n",
    "        data_qubits = data_qubits,\n",
    "        readout=readout)\n",
    "\n",
    "    # Then add layers (experiment by adding more).\n",
    "    builder.add_layer(circuit, cirq.XX, \"xx1\")\n",
    "    builder.add_layer(circuit, cirq.ZZ, \"zz1\")\n",
    "\n",
    "    # Finally, prepare the readout qubit.\n",
    "    circuit.append(cirq.H(readout))\n",
    "\n",
    "    return circuit, cirq.Z(readout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 687,
     "status": "ok",
     "timestamp": 1594250014033,
     "user": {
      "displayName": "Nanqing Dong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggv6drEa6aTWlDrOv1fiTXBiKpXDvz1Pm8yIE8vgg=s64",
      "userId": "15665172758075970106"
     },
     "user_tz": -60
    },
    "id": "sg7cqnZNAnzQ"
   },
   "outputs": [],
   "source": [
    "THRESHOLD = 0.5\n",
    "\n",
    "def filter(x, y, a=3, b=6):\n",
    "    keep = (y == a) | (y == b)\n",
    "    x, y = x[keep], y[keep]\n",
    "    y = y == a\n",
    "    return x,y\n",
    "\n",
    "\n",
    "def remove_ambiguous(xs, ys, a=3, b=6):\n",
    "    mapping = collections.defaultdict(set)\n",
    "    # Determine the set of labels for each unique image:\n",
    "    for x,y in zip(xs,ys):\n",
    "       mapping[tuple(x.flatten())].add(y)\n",
    "    \n",
    "    new_x = []\n",
    "    new_y = []\n",
    "    for x,y in zip(xs, ys):\n",
    "      labels = mapping[tuple(x.flatten())]\n",
    "      if len(labels) == 1:\n",
    "          new_x.append(x)\n",
    "          new_y.append(list(labels)[0])\n",
    "      else:\n",
    "          # Throw out images that match more than one label.\n",
    "          pass\n",
    "    \n",
    "    num_a = sum(1 for value in mapping.values() if True in value)\n",
    "    num_b = sum(1 for value in mapping.values() if False in value)\n",
    "    num_both = sum(1 for value in mapping.values() if len(value) == 2)\n",
    "\n",
    "    print(\"Number of unique images:\", len(mapping.values()))\n",
    "    print(\"Number of {}s: \".format(a), num_a)\n",
    "    print(\"Number of {}s: \".format(b), num_b)\n",
    "    print(\"Number of contradictory images: \", num_both)\n",
    "    print(\"Initial number of examples: \", len(xs))\n",
    "    print(\"Remaining non-contradictory examples: \", len(new_x))\n",
    "    print()\n",
    "    \n",
    "    return np.array(new_x), np.array(new_y)\n",
    "\n",
    "\n",
    "def convert_to_circuit(image, h):\n",
    "    \"\"\"Encode truncated classical image into quantum datapoint.\"\"\"\n",
    "    values = np.ndarray.flatten(image)\n",
    "    qubits = cirq.GridQubit.rect(h, h)\n",
    "    circuit = cirq.Circuit()\n",
    "    for i, value in enumerate(values):\n",
    "        if value:\n",
    "            circuit.append(cirq.X(qubits[i]))\n",
    "    return circuit\n",
    "\n",
    "\n",
    "def prepare_quantum_data(x, y, num_qubits, a=3, b=6, invert=False):\n",
    "    h = get_edge(num_qubits)\n",
    "    x = x[..., np.newaxis] / 255.0\n",
    "    x, y = filter(x, y, a, b)\n",
    "    x_small = tf.image.resize(x, (h, h)).numpy()\n",
    "\n",
    "    x_bin = np.array(x_small > THRESHOLD, dtype=np.float32)\n",
    "\n",
    "    x_nocon, y_nocon = remove_ambiguous(x_bin, y, a, b)\n",
    "\n",
    "    if invert:\n",
    "        x_nocon = 1 - x_nocon\n",
    "\n",
    "    x_circ = [convert_to_circuit(x, h) for x in x_nocon]\n",
    "    x_tfcirc = tfq.convert_to_tensor(x_circ)\n",
    "\n",
    "    y_nocon = 2.0*y_nocon-1.0\n",
    "\n",
    "    return x_tfcirc, y_nocon\n",
    "\n",
    "\n",
    "def prepare_classical_data(x, y, num_qubits, a=3, b=6, invert=False):\n",
    "    h = get_edge(num_qubits)\n",
    "    x = x[..., np.newaxis] / 255.0\n",
    "    x, y = filter(x, y, a, b)\n",
    "    x_small = tf.image.resize(x, (h, h)).numpy()\n",
    "\n",
    "    x_bin = np.array(x_small > THRESHOLD, dtype=np.float32)\n",
    "\n",
    "    x_nocon, y_nocon = remove_ambiguous(x_bin, y, a, b)\n",
    "\n",
    "    if invert:\n",
    "        x_nocon = 1 - x_nocon\n",
    "\n",
    "    return x_nocon, y_nocon\n",
    "\n",
    "\n",
    "def get_mnist_train(num_qubits, a, b, quantum=True):\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    if quantum:\n",
    "        x_train_tfcirc, y_train_hinge = prepare_quantum_data(x_train, y_train, \n",
    "                                                       num_qubits, a, b)\n",
    "        return x_train_tfcirc, y_train_hinge\n",
    "    else:\n",
    "        x_train_bin, y_train = prepare_classical_data(x_train, y_train, \n",
    "                                                      num_qubits, a, b)\n",
    "        return x_train_bin, y_train\n",
    "        \n",
    "\n",
    "def get_mnist_test(num_qubits, a, b, quantum=True, invert=True):\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    \n",
    "    if quantum:\n",
    "        x_test_tfcirc, y_test_hinge = prepare_quantum_data(x_test, y_test, \n",
    "                                                           num_qubits, a, b,\n",
    "                                                           invert=invert)\n",
    "        return x_test_tfcirc, y_test_hinge\n",
    "    else: \n",
    "        x_test_bin, y_test = prepare_classical_data(x_test, y_test, \n",
    "                                                    num_qubits, a, b,\n",
    "                                                    invert=invert)\n",
    "        return x_test_bin, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 862,
     "status": "ok",
     "timestamp": 1594248296304,
     "user": {
      "displayName": "Nanqing Dong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggv6drEa6aTWlDrOv1fiTXBiKpXDvz1Pm8yIE8vgg=s64",
      "userId": "15665172758075970106"
     },
     "user_tz": -60
    },
    "id": "jsd9eaF1id9d"
   },
   "outputs": [],
   "source": [
    "def hinge_accuracy(y_true, y_pred):\n",
    "    y_true = tf.squeeze(y_true) > 0.0\n",
    "    y_pred = tf.squeeze(y_pred) > 0.0\n",
    "    result = tf.cast(y_true == y_pred, tf.float32)\n",
    "    return tf.reduce_mean(result)\n",
    "\n",
    "\n",
    "def get_edge(num_qubits):\n",
    "    h = int(np.sqrt(num_qubits))\n",
    "    assert h**2 == num_qubits, 'num_qubits is not a perfect square of an integer!'\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 850,
     "status": "ok",
     "timestamp": 1594248296306,
     "user": {
      "displayName": "Nanqing Dong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggv6drEa6aTWlDrOv1fiTXBiKpXDvz1Pm8yIE8vgg=s64",
      "userId": "15665172758075970106"
     },
     "user_tz": -60
    },
    "id": "Hjvf_6woOS25"
   },
   "outputs": [],
   "source": [
    "def create_fair_classical_model(num_qubits=16, model_name='dnn'):\n",
    "    # A simple model based off LeNet from https://keras.io/examples/mnist_cnn/\n",
    "    h = get_edge(num_qubits)\n",
    "    model = tf.keras.Sequential(name=model_name)\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(h,h,1)))\n",
    "    model.add(tf.keras.layers.Dense(2, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Uc3Rnlz8R_5"
   },
   "source": [
    "# 3 vs 6 (16 Qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cLyTjzf7X68g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique images: 193\n",
      "Number of 3s:  125\n",
      "Number of 6s:  113\n",
      "Number of contradictory images:  45\n",
      "Initial number of examples:  12049\n",
      "Remaining non-contradictory examples:  3649\n",
      "\n",
      "Number of unique images: 115\n",
      "Number of 3s:  65\n",
      "Number of 6s:  72\n",
      "Number of contradictory images:  22\n",
      "Initial number of examples:  1968\n",
      "Remaining non-contradictory examples:  890\n",
      "\n",
      "Train on 3649 samples, validate on 890 samples\n",
      "Epoch 1/3\n",
      "3649/3649 [==============================] - 503s 138ms/sample - loss: 0.9677 - hinge_accuracy: 0.6266 - val_loss: 0.8902 - val_hinge_accuracy: 0.5980\n",
      "Epoch 2/3\n",
      "3649/3649 [==============================] - 502s 138ms/sample - loss: 0.5377 - hinge_accuracy: 0.8913 - val_loss: 0.3693 - val_hinge_accuracy: 0.9292\n",
      "Epoch 3/3\n",
      "3649/3649 [==============================] - 502s 137ms/sample - loss: 0.1691 - hinge_accuracy: 0.9565 - val_loss: 0.1704 - val_hinge_accuracy: 0.9540\n",
      "The training time for 3 epochs is 1506.4577417373657\n",
      "890/890 [==============================] - 4s 4ms/sample - loss: 0.1704 - hinge_accuracy: 0.9540\n"
     ]
    }
   ],
   "source": [
    "## Encapsulation\n",
    "x_train_tfcirc, y_train_hinge = get_mnist_train(16, a=3, b=6)\n",
    "x_test_tfcirc, y_test_hinge = get_mnist_test(16, a=3, b=6, invert=False)\n",
    "\n",
    "model_circuit, model_readout = create_quantum_model(num_qubits=16)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(), dtype=tf.string),\n",
    "        tfq.layers.PQC(model_circuit, model_readout),\n",
    "        ], name='qnn')\n",
    "\n",
    "model.compile(\n",
    "        loss=tf.keras.losses.Hinge(),\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=[hinge_accuracy])\n",
    "\n",
    "start = time.time()\n",
    "qnn_history = model.fit(\n",
    "        x_train_tfcirc, y_train_hinge,\n",
    "        batch_size=32,\n",
    "        epochs=3,\n",
    "        verbose=2,\n",
    "        validation_data=(x_test_tfcirc, y_test_hinge))\n",
    "end = time.time()\n",
    "print('The training time for 3 epochs is {}'.format(end - start))\n",
    "\n",
    "qnn_results = model.evaluate(x_test_tfcirc, y_test_hinge)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4321513,
     "status": "ok",
     "timestamp": 1594157419778,
     "user": {
      "displayName": "Nanqing Dong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggv6drEa6aTWlDrOv1fiTXBiKpXDvz1Pm8yIE8vgg=s64",
      "userId": "15665172758075970106"
     },
     "user_tz": -60
    },
    "id": "dojX9MyLD1Rq",
    "outputId": "378f71f9-241a-4b8b-dc90-541310136432"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique images: 193\n",
      "Number of 3s:  125\n",
      "Number of 6s:  113\n",
      "Number of contradictory images:  45\n",
      "Initial number of examples:  12049\n",
      "Remaining non-contradictory examples:  3649\n",
      "\n",
      "Number of unique images: 115\n",
      "Number of 3s:  65\n",
      "Number of 6s:  72\n",
      "Number of contradictory images:  22\n",
      "Initial number of examples:  1968\n",
      "Remaining non-contradictory examples:  890\n",
      "\n",
      "Train on 3649 samples, validate on 890 samples\n",
      "Epoch 1/5\n",
      "3649/3649 [==============================] - 505s 138ms/sample - loss: 0.8565 - hinge_accuracy: 0.8701 - val_loss: 0.6763 - val_hinge_accuracy: 0.7787\n",
      "Epoch 2/5\n",
      "3649/3649 [==============================] - 506s 139ms/sample - loss: 0.3061 - hinge_accuracy: 0.9476 - val_loss: 0.2243 - val_hinge_accuracy: 0.9588\n",
      "Epoch 3/5\n",
      "3649/3649 [==============================] - 506s 139ms/sample - loss: 0.1344 - hinge_accuracy: 0.9818 - val_loss: 0.1629 - val_hinge_accuracy: 0.9830\n",
      "Epoch 4/5\n",
      "3649/3649 [==============================] - 506s 139ms/sample - loss: 0.0787 - hinge_accuracy: 0.9946 - val_loss: 0.1708 - val_hinge_accuracy: 0.9933\n",
      "Epoch 5/5\n",
      "3649/3649 [==============================] - 507s 139ms/sample - loss: 0.0509 - hinge_accuracy: 0.9970 - val_loss: 0.1960 - val_hinge_accuracy: 0.9955\n",
      "The training time for 5 epochs is 2530.5152316093445\n",
      "890/890 [==============================] - 4s 5ms/sample - loss: 0.1960 - hinge_accuracy: 0.9955\n"
     ]
    }
   ],
   "source": [
    "## Encapsulation QNN Domain Adaptation\n",
    "try:\n",
    "    del model_circuit, model_readout, model\n",
    "except:\n",
    "    pass\n",
    "x_train_tfcirc, y_train_hinge = get_mnist_train(16, a=3, b=6)\n",
    "x_test_tfcirc, y_test_hinge = get_mnist_test(16, a=3, b=6)\n",
    "\n",
    "model_circuit, model_readout = create_quantum_model(num_qubits=16)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(), dtype=tf.string),\n",
    "    tfq.layers.PQC(model_circuit, model_readout),\n",
    "    ], name='qnn_da')\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.Hinge(),\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[hinge_accuracy])\n",
    "\n",
    "start = time.time()\n",
    "qnn_history = model.fit(\n",
    "    x_train_tfcirc, y_train_hinge,\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    verbose=2,\n",
    "    validation_data=(x_test_tfcirc, y_test_hinge))\n",
    "end = time.time()\n",
    "print('The training time for 5 epochs is {}'.format(end - start))\n",
    "\n",
    "qnn_results = model.evaluate(x_test_tfcirc, y_test_hinge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M9VStd7H7G_3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique images: 193\n",
      "Number of 3s:  125\n",
      "Number of 6s:  113\n",
      "Number of contradictory images:  45\n",
      "Initial number of examples:  12049\n",
      "Remaining non-contradictory examples:  3649\n",
      "\n",
      "Number of unique images: 115\n",
      "Number of 3s:  65\n",
      "Number of 6s:  72\n",
      "Number of contradictory images:  22\n",
      "Initial number of examples:  1968\n",
      "Remaining non-contradictory examples:  890\n",
      "\n",
      "Model: \"dnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 37\n",
      "Trainable params: 37\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3649 samples, validate on 890 samples\n",
      "Epoch 1/20\n",
      "3649/3649 - 1s - loss: 0.5590 - accuracy: 0.8622 - val_loss: 0.6425 - val_accuracy: 0.8067\n",
      "Epoch 2/20\n",
      "3649/3649 - 0s - loss: 0.4383 - accuracy: 0.9318 - val_loss: 0.5908 - val_accuracy: 0.8146\n",
      "Epoch 3/20\n",
      "3649/3649 - 0s - loss: 0.3747 - accuracy: 0.9466 - val_loss: 0.5363 - val_accuracy: 0.8393\n",
      "Epoch 4/20\n",
      "3649/3649 - 0s - loss: 0.3254 - accuracy: 0.9578 - val_loss: 0.4783 - val_accuracy: 0.8640\n",
      "Epoch 5/20\n",
      "3649/3649 - 0s - loss: 0.2863 - accuracy: 0.9847 - val_loss: 0.4281 - val_accuracy: 0.9787\n",
      "Epoch 6/20\n",
      "3649/3649 - 0s - loss: 0.2549 - accuracy: 0.9890 - val_loss: 0.3918 - val_accuracy: 0.9798\n",
      "Epoch 7/20\n",
      "3649/3649 - 0s - loss: 0.2285 - accuracy: 0.9893 - val_loss: 0.3602 - val_accuracy: 0.9798\n",
      "Epoch 8/20\n",
      "3649/3649 - 0s - loss: 0.2046 - accuracy: 0.9915 - val_loss: 0.3275 - val_accuracy: 0.9843\n",
      "Epoch 9/20\n",
      "3649/3649 - 0s - loss: 0.1839 - accuracy: 0.9937 - val_loss: 0.2987 - val_accuracy: 0.9854\n",
      "Epoch 10/20\n",
      "3649/3649 - 0s - loss: 0.1661 - accuracy: 0.9945 - val_loss: 0.2726 - val_accuracy: 0.9865\n",
      "Epoch 11/20\n",
      "3649/3649 - 0s - loss: 0.1506 - accuracy: 0.9962 - val_loss: 0.2523 - val_accuracy: 0.9876\n",
      "Epoch 12/20\n",
      "3649/3649 - 0s - loss: 0.1373 - accuracy: 0.9978 - val_loss: 0.2355 - val_accuracy: 0.9876\n",
      "Epoch 13/20\n",
      "3649/3649 - 0s - loss: 0.1257 - accuracy: 0.9981 - val_loss: 0.2206 - val_accuracy: 0.9876\n",
      "Epoch 14/20\n",
      "3649/3649 - 0s - loss: 0.1154 - accuracy: 0.9984 - val_loss: 0.2060 - val_accuracy: 0.9865\n",
      "Epoch 15/20\n",
      "3649/3649 - 0s - loss: 0.1062 - accuracy: 0.9984 - val_loss: 0.1943 - val_accuracy: 0.9865\n",
      "Epoch 16/20\n",
      "3649/3649 - 0s - loss: 0.0979 - accuracy: 0.9984 - val_loss: 0.1820 - val_accuracy: 0.9865\n",
      "Epoch 17/20\n",
      "3649/3649 - 0s - loss: 0.0905 - accuracy: 0.9984 - val_loss: 0.1703 - val_accuracy: 0.9978\n",
      "Epoch 18/20\n",
      "3649/3649 - 0s - loss: 0.0838 - accuracy: 0.9995 - val_loss: 0.1600 - val_accuracy: 0.9978\n",
      "Epoch 19/20\n",
      "3649/3649 - 0s - loss: 0.0778 - accuracy: 0.9997 - val_loss: 0.1510 - val_accuracy: 0.9978\n",
      "Epoch 20/20\n",
      "3649/3649 - 0s - loss: 0.0723 - accuracy: 1.0000 - val_loss: 0.1416 - val_accuracy: 0.9989\n",
      "The training time for 20 epochs is 7.233209133148193\n",
      "890/890 [==============================] - 0s 54us/sample - loss: 0.1416 - accuracy: 0.9989\n"
     ]
    }
   ],
   "source": [
    "# Encapsulation\n",
    "x_train_bin, y_train = get_mnist_train(16, a=3, b=6, quantum=False)\n",
    "x_test_bin, y_test = get_mnist_test(16, a=3, b=6, quantum=False, invert=False)\n",
    "\n",
    "model = create_fair_classical_model(num_qubits=16)\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "start = time.time()\n",
    "model.fit(x_train_bin,\n",
    "          y_train,\n",
    "          batch_size=32,\n",
    "          epochs=20,\n",
    "          verbose=2,\n",
    "          validation_data=(x_test_bin, y_test))\n",
    "end = time.time()\n",
    "print('The training time for 20 epochs is {}'.format(end - start))\n",
    "\n",
    "fair_nn_results = model.evaluate(x_test_bin, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique images: 193\n",
      "Number of 3s:  125\n",
      "Number of 6s:  113\n",
      "Number of contradictory images:  45\n",
      "Initial number of examples:  12049\n",
      "Remaining non-contradictory examples:  3649\n",
      "\n",
      "Number of unique images: 115\n",
      "Number of 3s:  65\n",
      "Number of 6s:  72\n",
      "Number of contradictory images:  22\n",
      "Initial number of examples:  1968\n",
      "Remaining non-contradictory examples:  890\n",
      "\n",
      "Model: \"dnn_da\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 37\n",
      "Trainable params: 37\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3649 samples, validate on 890 samples\n",
      "Epoch 1/20\n",
      "3649/3649 - 1s - loss: 0.6472 - accuracy: 0.5256 - val_loss: 0.6676 - val_accuracy: 0.8315\n",
      "Epoch 2/20\n",
      "3649/3649 - 0s - loss: 0.5807 - accuracy: 0.7490 - val_loss: 0.6316 - val_accuracy: 0.9067\n",
      "Epoch 3/20\n",
      "3649/3649 - 0s - loss: 0.5172 - accuracy: 0.8517 - val_loss: 0.5872 - val_accuracy: 0.9404\n",
      "Epoch 4/20\n",
      "3649/3649 - 0s - loss: 0.4520 - accuracy: 0.9030 - val_loss: 0.5420 - val_accuracy: 0.9416\n",
      "Epoch 5/20\n",
      "3649/3649 - 0s - loss: 0.3944 - accuracy: 0.9055 - val_loss: 0.4892 - val_accuracy: 0.9820\n",
      "Epoch 6/20\n",
      "3649/3649 - 0s - loss: 0.3428 - accuracy: 0.9811 - val_loss: 0.4191 - val_accuracy: 0.9865\n",
      "Epoch 7/20\n",
      "3649/3649 - 0s - loss: 0.2574 - accuracy: 0.9844 - val_loss: 0.2625 - val_accuracy: 0.9843\n",
      "Epoch 8/20\n",
      "3649/3649 - 0s - loss: 0.1655 - accuracy: 0.9860 - val_loss: 0.1674 - val_accuracy: 0.9921\n",
      "Epoch 9/20\n",
      "3649/3649 - 0s - loss: 0.1126 - accuracy: 0.9868 - val_loss: 0.1191 - val_accuracy: 0.9921\n",
      "Epoch 10/20\n",
      "3649/3649 - 0s - loss: 0.0829 - accuracy: 0.9868 - val_loss: 0.0936 - val_accuracy: 0.9921\n",
      "Epoch 11/20\n",
      "3649/3649 - 0s - loss: 0.0638 - accuracy: 0.9879 - val_loss: 0.0790 - val_accuracy: 0.9944\n",
      "Epoch 12/20\n",
      "3649/3649 - 0s - loss: 0.0511 - accuracy: 0.9907 - val_loss: 0.0679 - val_accuracy: 0.9944\n",
      "Epoch 13/20\n",
      "3649/3649 - 0s - loss: 0.0419 - accuracy: 0.9912 - val_loss: 0.0598 - val_accuracy: 0.9944\n",
      "Epoch 14/20\n",
      "3649/3649 - 0s - loss: 0.0349 - accuracy: 0.9921 - val_loss: 0.0538 - val_accuracy: 0.9966\n",
      "Epoch 15/20\n",
      "3649/3649 - 0s - loss: 0.0294 - accuracy: 0.9937 - val_loss: 0.0494 - val_accuracy: 0.9966\n",
      "Epoch 16/20\n",
      "3649/3649 - 0s - loss: 0.0252 - accuracy: 0.9964 - val_loss: 0.0455 - val_accuracy: 0.9966\n",
      "Epoch 17/20\n",
      "3649/3649 - 0s - loss: 0.0217 - accuracy: 0.9964 - val_loss: 0.0433 - val_accuracy: 0.9966\n",
      "Epoch 18/20\n",
      "3649/3649 - 0s - loss: 0.0188 - accuracy: 0.9975 - val_loss: 0.0402 - val_accuracy: 0.9978\n",
      "Epoch 19/20\n",
      "3649/3649 - 0s - loss: 0.0164 - accuracy: 0.9981 - val_loss: 0.0379 - val_accuracy: 0.9978\n",
      "Epoch 20/20\n",
      "3649/3649 - 0s - loss: 0.0144 - accuracy: 0.9981 - val_loss: 0.0362 - val_accuracy: 0.9966\n",
      "The training time for 20 epochs is 6.9922239780426025\n",
      "890/890 [==============================] - 0s 52us/sample - loss: 0.0362 - accuracy: 0.9966\n"
     ]
    }
   ],
   "source": [
    "# Encapsulation\n",
    "x_train_bin, y_train = get_mnist_train(16, a=3, b=6, quantum=False)\n",
    "x_test_bin, y_test = get_mnist_test(16, a=3, b=6, quantum=False, invert=False)\n",
    "\n",
    "model = create_fair_classical_model(num_qubits=16, model_name='dnn_da')\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "start = time.time()\n",
    "model.fit(x_train_bin,\n",
    "          y_train,\n",
    "          batch_size=32,\n",
    "          epochs=20,\n",
    "          verbose=2,\n",
    "          validation_data=(x_test_bin, y_test))\n",
    "end = time.time()\n",
    "print('The training time for 20 epochs is {}'.format(end - start))\n",
    "\n",
    "fair_nn_results = model.evaluate(x_test_bin, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 848,
     "status": "ok",
     "timestamp": 1594248296307,
     "user": {
      "displayName": "Nanqing Dong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggv6drEa6aTWlDrOv1fiTXBiKpXDvz1Pm8yIE8vgg=s64",
      "userId": "15665172758075970106"
     },
     "user_tz": -60
    },
    "id": "t_xgd6WnsWeE"
   },
   "outputs": [],
   "source": [
    "def train_dnn(x_train, y_train, x_test, y_test, batch_size=32, num_steps=1000, intval=10):\n",
    "    num_intval = num_steps // intval\n",
    "    num_train = x_train.shape[0]\n",
    "\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (x_train, y_train)).repeat().shuffle(num_train).batch(batch_size)\n",
    "    \n",
    "    train_eval_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        x_batch, y_batch = next(iter(train_dataset))\n",
    "        train_step(x_batch, y_batch)\n",
    "\n",
    "        if step % intval == 0:\n",
    "            train_loss_tracker = tf.keras.metrics.Mean()\n",
    "            train_acc_tracker = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "            for x, y in train_eval_dataset:\n",
    "                logits = model(x, training=False)\n",
    "                loss = loss_function(y, logits)\n",
    "                train_loss_tracker.update_state(loss)\n",
    "                train_acc_tracker.update_state(y, logits)\n",
    "        \n",
    "            avg_loss = train_loss_tracker.result()\n",
    "            avg_acc = train_acc_tracker.result()\n",
    "            train_loss.append(avg_loss)\n",
    "            train_acc.append(avg_acc)\n",
    "\n",
    "            test_loss_tracker = tf.keras.metrics.Mean()\n",
    "            test_acc_tracker = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "            for x, y in test_dataset:\n",
    "                logits = model(x, training=False)\n",
    "                loss = loss_function(y, logits)\n",
    "                test_loss_tracker.update_state(loss)\n",
    "                test_acc_tracker.update_state(y, logits)\n",
    "\n",
    "            avg_test_loss = test_loss_tracker.result()\n",
    "            avg_test_acc = test_acc_tracker.result()\n",
    "            test_loss.append(avg_test_loss)\n",
    "            test_acc.append(avg_test_acc)\n",
    "                \n",
    "        if step % 100 == 0:\n",
    "            print(\"Step {:03d}: Train Loss: {:.3f}, Train Accuracy: {:.3%}, Test Loss: {:.3f}, Test Accuracy: {:.3%}\".format(\n",
    "                step, avg_loss, avg_acc, avg_test_loss, avg_test_acc))\n",
    "            \n",
    "    return np.array([train_loss, train_acc, test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 751,
     "status": "ok",
     "timestamp": 1594174349080,
     "user": {
      "displayName": "Nanqing Dong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggv6drEa6aTWlDrOv1fiTXBiKpXDvz1Pm8yIE8vgg=s64",
      "userId": "15665172758075970106"
     },
     "user_tz": -60
    },
    "id": "NBJeeGVe2rsg"
   },
   "outputs": [],
   "source": [
    "class HingeAccuracy:\n",
    "    def __init__(self):\n",
    "        self.total = 0.0\n",
    "        self.count = 0.0\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.total = 0.0\n",
    "        self.count = 0.0\n",
    "\n",
    "    def result(self):\n",
    "        return self.count / self.total\n",
    "\n",
    "    def update_state(self, y_true, y_pred):\n",
    "        y_true = tf.squeeze(y_true) > 0.0\n",
    "        y_pred = tf.squeeze(y_pred) > 0.0\n",
    "        try:\n",
    "            self.total += y_true.shape[0]\n",
    "        except:\n",
    "            self.total += 1\n",
    "        self.count += tf.reduce_sum(tf.cast(y_true == y_pred, tf.float32))\n",
    "\n",
    "\n",
    "def train_qnn(x_train, y_train, x_test, y_test, batch_size=32, num_steps=1000, \n",
    "              intval=10):\n",
    "    num_intval = num_steps // intval\n",
    "    num_train = x_train.shape[0]\n",
    "\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (x_train, y_train)).repeat().shuffle(num_train).batch(batch_size)\n",
    "    \n",
    "    train_eval_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        x_batch, y_batch = next(iter(train_dataset))\n",
    "        train_step(x_batch, y_batch)\n",
    "\n",
    "        if step % intval == 0:\n",
    "            train_loss_tracker = tf.keras.metrics.Mean()\n",
    "            train_acc_tracker = HingeAccuracy()\n",
    "\n",
    "            for x, y in train_eval_dataset:\n",
    "                logits = model(x, training=False)\n",
    "                loss = loss_function(y, logits)\n",
    "                train_loss_tracker.update_state(loss)\n",
    "                train_acc_tracker.update_state(y, logits)\n",
    "        \n",
    "            avg_loss = train_loss_tracker.result()\n",
    "            avg_acc = train_acc_tracker.result()\n",
    "            train_loss.append(avg_loss)\n",
    "            train_acc.append(avg_acc)\n",
    "\n",
    "            test_loss_tracker = tf.keras.metrics.Mean()\n",
    "            test_acc_tracker = HingeAccuracy()\n",
    "\n",
    "            for x, y in test_dataset:\n",
    "                logits = model(x, training=False)\n",
    "                loss = loss_function(y, logits)\n",
    "                test_loss_tracker.update_state(loss)\n",
    "                test_acc_tracker.update_state(y, logits)\n",
    "\n",
    "            avg_test_loss = test_loss_tracker.result()\n",
    "            avg_test_acc = test_acc_tracker.result()\n",
    "            test_loss.append(avg_test_loss)\n",
    "            test_acc.append(avg_test_acc)\n",
    "                \n",
    "        if step % 100 == 0:\n",
    "            print(\"Step {:03d}: Train Loss: {:.3f}, Train Accuracy: {:.3%}, Test Loss: {:.3f}, Test Accuracy: {:.3%}\".format(\n",
    "                step, avg_loss, avg_acc, avg_test_loss, avg_test_acc))\n",
    "            \n",
    "    return np.array([train_loss, train_acc, test_acc])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itix1XkH6dQh"
   },
   "source": [
    "### DNN  16 Qubits 3 vs 6  Supervised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 474524,
     "status": "ok",
     "timestamp": 1594250538948,
     "user": {
      "displayName": "Nanqing Dong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggv6drEa6aTWlDrOv1fiTXBiKpXDvz1Pm8yIE8vgg=s64",
      "userId": "15665172758075970106"
     },
     "user_tz": -60
    },
    "id": "6FUPFTZ621p1",
    "outputId": "d4797cbc-7061-4487-c1c2-568fda1aebdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique images: 193\n",
      "Number of 3s:  125\n",
      "Number of 6s:  113\n",
      "Number of contradictory images:  45\n",
      "Initial number of examples:  12049\n",
      "Remaining non-contradictory examples:  3649\n",
      "\n",
      "Number of unique images: 115\n",
      "Number of 3s:  65\n",
      "Number of 6s:  72\n",
      "Number of contradictory images:  22\n",
      "Initial number of examples:  1968\n",
      "Remaining non-contradictory examples:  890\n",
      "\n",
      "Releasae memory!\n",
      "dnn_1\n",
      "Step 100: Train Loss: 0.747, Train Accuracy: 43.641%, Test Loss: 0.701, Test Accuracy: 62.766%\n",
      "Step 200: Train Loss: 0.657, Train Accuracy: 43.641%, Test Loss: 0.644, Test Accuracy: 62.766%\n",
      "Step 300: Train Loss: 0.595, Train Accuracy: 43.641%, Test Loss: 0.588, Test Accuracy: 62.766%\n",
      "Step 400: Train Loss: 0.533, Train Accuracy: 43.641%, Test Loss: 0.526, Test Accuracy: 62.766%\n",
      "Step 500: Train Loss: 0.470, Train Accuracy: 43.641%, Test Loss: 0.461, Test Accuracy: 62.766%\n",
      "Step 600: Train Loss: 0.418, Train Accuracy: 43.641%, Test Loss: 0.392, Test Accuracy: 62.766%\n",
      "Step 700: Train Loss: 0.375, Train Accuracy: 91.033%, Test Loss: 0.345, Test Accuracy: 91.973%\n",
      "Step 800: Train Loss: 0.322, Train Accuracy: 91.060%, Test Loss: 0.287, Test Accuracy: 92.084%\n",
      "Step 900: Train Loss: 0.280, Train Accuracy: 91.196%, Test Loss: 0.244, Test Accuracy: 92.196%\n",
      "Step 1000: Train Loss: 0.245, Train Accuracy: 96.005%, Test Loss: 0.215, Test Accuracy: 95.012%\n",
      "Releasae memory!\n",
      "dnn_2\n",
      "Step 100: Train Loss: 0.494, Train Accuracy: 74.592%, Test Loss: 0.542, Test Accuracy: 82.151%\n",
      "Step 200: Train Loss: 0.389, Train Accuracy: 88.777%, Test Loss: 0.444, Test Accuracy: 92.162%\n",
      "Step 300: Train Loss: 0.294, Train Accuracy: 94.293%, Test Loss: 0.350, Test Accuracy: 95.398%\n",
      "Step 400: Train Loss: 0.221, Train Accuracy: 95.217%, Test Loss: 0.271, Test Accuracy: 96.291%\n",
      "Step 500: Train Loss: 0.170, Train Accuracy: 96.005%, Test Loss: 0.218, Test Accuracy: 96.961%\n",
      "Step 600: Train Loss: 0.133, Train Accuracy: 98.451%, Test Loss: 0.179, Test Accuracy: 98.747%\n",
      "Step 700: Train Loss: 0.106, Train Accuracy: 98.696%, Test Loss: 0.150, Test Accuracy: 98.747%\n",
      "Step 800: Train Loss: 0.086, Train Accuracy: 98.804%, Test Loss: 0.126, Test Accuracy: 98.747%\n",
      "Step 900: Train Loss: 0.071, Train Accuracy: 98.940%, Test Loss: 0.110, Test Accuracy: 98.747%\n",
      "Step 1000: Train Loss: 0.060, Train Accuracy: 99.375%, Test Loss: 0.097, Test Accuracy: 99.193%\n",
      "Releasae memory!\n",
      "dnn_3\n",
      "Step 100: Train Loss: 0.549, Train Accuracy: 43.641%, Test Loss: 0.541, Test Accuracy: 62.766%\n",
      "Step 200: Train Loss: 0.479, Train Accuracy: 43.641%, Test Loss: 0.461, Test Accuracy: 62.766%\n",
      "Step 300: Train Loss: 0.417, Train Accuracy: 43.641%, Test Loss: 0.388, Test Accuracy: 62.766%\n",
      "Step 400: Train Loss: 0.368, Train Accuracy: 43.641%, Test Loss: 0.332, Test Accuracy: 62.766%\n",
      "Step 500: Train Loss: 0.327, Train Accuracy: 88.750%, Test Loss: 0.293, Test Accuracy: 90.771%\n",
      "Step 600: Train Loss: 0.294, Train Accuracy: 94.484%, Test Loss: 0.260, Test Accuracy: 94.900%\n",
      "Step 700: Train Loss: 0.267, Train Accuracy: 96.087%, Test Loss: 0.237, Test Accuracy: 95.458%\n",
      "Step 800: Train Loss: 0.243, Train Accuracy: 96.848%, Test Loss: 0.218, Test Accuracy: 96.016%\n",
      "Step 900: Train Loss: 0.222, Train Accuracy: 97.799%, Test Loss: 0.200, Test Accuracy: 96.463%\n",
      "Step 1000: Train Loss: 0.204, Train Accuracy: 98.234%, Test Loss: 0.185, Test Accuracy: 96.712%\n",
      "Releasae memory!\n",
      "dnn_4\n",
      "Step 100: Train Loss: 0.693, Train Accuracy: 43.641%, Test Loss: 0.711, Test Accuracy: 62.766%\n",
      "Step 200: Train Loss: 0.682, Train Accuracy: 43.641%, Test Loss: 0.707, Test Accuracy: 62.766%\n",
      "Step 300: Train Loss: 0.658, Train Accuracy: 44.049%, Test Loss: 0.681, Test Accuracy: 63.101%\n",
      "Step 400: Train Loss: 0.586, Train Accuracy: 53.261%, Test Loss: 0.616, Test Accuracy: 68.458%\n",
      "Step 500: Train Loss: 0.371, Train Accuracy: 95.245%, Test Loss: 0.451, Test Accuracy: 94.703%\n",
      "Step 600: Train Loss: 0.224, Train Accuracy: 98.913%, Test Loss: 0.304, Test Accuracy: 99.193%\n",
      "Step 700: Train Loss: 0.146, Train Accuracy: 99.348%, Test Loss: 0.213, Test Accuracy: 99.416%\n",
      "Step 800: Train Loss: 0.102, Train Accuracy: 99.538%, Test Loss: 0.159, Test Accuracy: 99.528%\n",
      "Step 900: Train Loss: 0.076, Train Accuracy: 99.783%, Test Loss: 0.126, Test Accuracy: 99.639%\n",
      "Step 1000: Train Loss: 0.059, Train Accuracy: 99.783%, Test Loss: 0.104, Test Accuracy: 99.639%\n",
      "Releasae memory!\n",
      "dnn_5\n",
      "Step 100: Train Loss: 0.559, Train Accuracy: 43.641%, Test Loss: 0.464, Test Accuracy: 62.766%\n",
      "Step 200: Train Loss: 0.473, Train Accuracy: 43.641%, Test Loss: 0.377, Test Accuracy: 62.766%\n",
      "Step 300: Train Loss: 0.401, Train Accuracy: 43.641%, Test Loss: 0.312, Test Accuracy: 62.766%\n",
      "Step 400: Train Loss: 0.353, Train Accuracy: 43.641%, Test Loss: 0.270, Test Accuracy: 62.766%\n",
      "Step 500: Train Loss: 0.316, Train Accuracy: 94.701%, Test Loss: 0.242, Test Accuracy: 94.566%\n",
      "Step 600: Train Loss: 0.286, Train Accuracy: 96.168%, Test Loss: 0.219, Test Accuracy: 95.682%\n",
      "Step 700: Train Loss: 0.260, Train Accuracy: 97.038%, Test Loss: 0.199, Test Accuracy: 96.016%\n",
      "Step 800: Train Loss: 0.238, Train Accuracy: 97.527%, Test Loss: 0.183, Test Accuracy: 96.240%\n",
      "Step 900: Train Loss: 0.218, Train Accuracy: 98.370%, Test Loss: 0.169, Test Accuracy: 96.798%\n",
      "Step 1000: Train Loss: 0.200, Train Accuracy: 98.397%, Test Loss: 0.157, Test Accuracy: 96.798%\n"
     ]
    }
   ],
   "source": [
    "x_train_bin, y_train = get_mnist_train(16, a=3, b=6, quantum=False)\n",
    "x_test_bin, y_test = get_mnist_test(16, a=3, b=6, quantum=False, invert=False)\n",
    "\n",
    "for REP in range(1, 6):\n",
    "    try:\n",
    "        del model, optimizer, loss_function\n",
    "        print('Releasae memory!')\n",
    "    except:\n",
    "        pass\n",
    "    finally:\n",
    "        model_name = 'dnn_{}'.format(REP)\n",
    "        print(model_name)\n",
    "        model = create_fair_classical_model(num_qubits=16, model_name=model_name)\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "        loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(x_batch, y_batch):\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x_batch, training=True)\n",
    "                loss = loss_function(y_batch, logits)\n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        data = train_dnn(x_train_bin, y_train, x_test_bin, y_test, batch_size=32, \n",
    "                     num_steps=1000, intval=10)\n",
    "\n",
    "        data = np.array(data)\n",
    "        data_path = os.path.join(data_dir, 'dnn_{}.npy'.format(REP))\n",
    "        np.save(data_path, data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4PsCrT67Wjj"
   },
   "source": [
    "### QNN 16 Qubits 3 vs 6 Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hm_pEpkv7aUk"
   },
   "outputs": [],
   "source": [
    "x_train_tfcirc, y_train_hinge = get_mnist_train(16, a=3, b=6)\n",
    "x_test_tfcirc, y_test_hinge = get_mnist_test(16, a=3, b=6, invert=False)\n",
    "\n",
    "for REP in range(2, 4):\n",
    "    try:\n",
    "        del model_circuit, model_readout, model, optimizer, loss_function\n",
    "        print('Release memory.')\n",
    "    except:\n",
    "        pass\n",
    "    finally:\n",
    "        model_name = 'qnn_{}'.format(REP)\n",
    "        print(model_name)\n",
    "        model_circuit, model_readout = create_quantum_model(num_qubits=16)\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(), dtype=tf.string),\n",
    "            tfq.layers.PQC(model_circuit, model_readout),\n",
    "            ], name=model_name)\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "        loss_function = tf.keras.losses.Hinge()\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(x_batch, y_batch):\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x_batch, training=True)\n",
    "                loss = loss_function(y_batch, logits)\n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        data = train_qnn(x_train_tfcirc, y_train_hinge, x_test_tfcirc, y_test_hinge, \n",
    "                         batch_size=32, num_steps=1000, intval=10)\n",
    "\n",
    "        data = np.array(data)\n",
    "        data_path = os.path.join(data_dir, 'qnn_{}.npy'.format(REP))\n",
    "        np.save(data_path, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_YX1JlxAmlcY"
   },
   "source": [
    "### DNN 16 Qubits 3 vs 6 Domain Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 192995,
     "status": "ok",
     "timestamp": 1594250731975,
     "user": {
      "displayName": "Nanqing Dong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggv6drEa6aTWlDrOv1fiTXBiKpXDvz1Pm8yIE8vgg=s64",
      "userId": "15665172758075970106"
     },
     "user_tz": -60
    },
    "id": "mqQ7T2NEmW82",
    "outputId": "520c6153-ed5f-40d9-b461-6f592578afc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique images: 193\n",
      "Number of 3s:  125\n",
      "Number of 6s:  113\n",
      "Number of contradictory images:  45\n",
      "Initial number of examples:  12049\n",
      "Remaining non-contradictory examples:  3649\n",
      "\n",
      "Number of unique images: 115\n",
      "Number of 3s:  65\n",
      "Number of 6s:  72\n",
      "Number of contradictory images:  22\n",
      "Initial number of examples:  1968\n",
      "Remaining non-contradictory examples:  890\n",
      "\n",
      "Releasae memory!\n",
      "dnn_da_4\n",
      "Step 100: Train Loss: 0.757, Train Accuracy: 43.641%, Test Loss: 0.701, Test Accuracy: 62.766%\n",
      "Step 200: Train Loss: 0.698, Train Accuracy: 43.641%, Test Loss: 0.711, Test Accuracy: 62.766%\n",
      "Step 300: Train Loss: 0.644, Train Accuracy: 43.641%, Test Loss: 0.724, Test Accuracy: 62.766%\n",
      "Step 400: Train Loss: 0.570, Train Accuracy: 43.641%, Test Loss: 0.805, Test Accuracy: 62.766%\n",
      "Step 500: Train Loss: 0.497, Train Accuracy: 43.641%, Test Loss: 0.911, Test Accuracy: 62.766%\n",
      "Step 600: Train Loss: 0.434, Train Accuracy: 43.641%, Test Loss: 1.036, Test Accuracy: 62.766%\n",
      "Step 700: Train Loss: 0.381, Train Accuracy: 89.647%, Test Loss: 1.177, Test Accuracy: 45.192%\n",
      "Step 800: Train Loss: 0.339, Train Accuracy: 92.880%, Test Loss: 1.320, Test Accuracy: 51.133%\n",
      "Step 900: Train Loss: 0.305, Train Accuracy: 94.810%, Test Loss: 1.463, Test Accuracy: 51.133%\n",
      "Step 1000: Train Loss: 0.276, Train Accuracy: 94.946%, Test Loss: 1.592, Test Accuracy: 45.081%\n",
      "Releasae memory!\n",
      "dnn_da_5\n",
      "Step 100: Train Loss: 0.737, Train Accuracy: 43.125%, Test Loss: 0.993, Test Accuracy: 42.033%\n",
      "Step 200: Train Loss: 0.702, Train Accuracy: 43.750%, Test Loss: 1.000, Test Accuracy: 41.561%\n",
      "Step 300: Train Loss: 0.680, Train Accuracy: 44.049%, Test Loss: 0.994, Test Accuracy: 46.137%\n",
      "Step 400: Train Loss: 0.659, Train Accuracy: 45.951%, Test Loss: 1.079, Test Accuracy: 34.341%\n",
      "Step 500: Train Loss: 0.608, Train Accuracy: 67.174%, Test Loss: 1.366, Test Accuracy: 35.173%\n",
      "Step 600: Train Loss: 0.471, Train Accuracy: 98.750%, Test Loss: 1.882, Test Accuracy: 34.890%\n",
      "Step 700: Train Loss: 0.365, Train Accuracy: 99.810%, Test Loss: 2.309, Test Accuracy: 19.463%\n",
      "Step 800: Train Loss: 0.165, Train Accuracy: 99.402%, Test Loss: 2.668, Test Accuracy: 3.709%\n",
      "Step 900: Train Loss: 0.098, Train Accuracy: 99.728%, Test Loss: 3.270, Test Accuracy: 3.735%\n",
      "Step 1000: Train Loss: 0.067, Train Accuracy: 99.755%, Test Loss: 3.756, Test Accuracy: 3.623%\n"
     ]
    }
   ],
   "source": [
    "x_train_bin, y_train = get_mnist_train(16, a=3, b=6, quantum=False)\n",
    "x_test_bin, y_test = get_mnist_test(16, a=3, b=6, quantum=False)\n",
    "\n",
    "for REP in range(4, 6):\n",
    "    try:\n",
    "        del model, optimizer, loss_function\n",
    "        print('Releasae memory!')\n",
    "    except:\n",
    "        pass\n",
    "    finally:\n",
    "        model_name = 'dnn_da_{}'.format(REP)\n",
    "        print(model_name)\n",
    "        model = create_fair_classical_model(num_qubits=16, model_name=model_name)\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "        loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(x_batch, y_batch):\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x_batch, training=True)\n",
    "                loss = loss_function(y_batch, logits)\n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        data = train_dnn(x_train_bin, y_train, x_test_bin, y_test, batch_size=32, \n",
    "                     num_steps=1000, intval=10)\n",
    "\n",
    "        data = np.array(data)\n",
    "        data_path = os.path.join(data_dir, 'dnn_da_{}.npy'.format(REP))\n",
    "        np.save(data_path, data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JjVdMOXtPp77"
   },
   "source": [
    "### QNN 16 Qubits 3 vs 6 Domain Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36517586,
     "status": "ok",
     "timestamp": 1594210872276,
     "user": {
      "displayName": "Nanqing Dong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggv6drEa6aTWlDrOv1fiTXBiKpXDvz1Pm8yIE8vgg=s64",
      "userId": "15665172758075970106"
     },
     "user_tz": -60
    },
    "id": "syMQ4hJqMT7g",
    "outputId": "8a742051-1ea4-4f89-a8bb-4bfbd0475d12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique images: 193\n",
      "Number of 3s:  125\n",
      "Number of 6s:  113\n",
      "Number of contradictory images:  45\n",
      "Initial number of examples:  12049\n",
      "Remaining non-contradictory examples:  3649\n",
      "\n",
      "Number of unique images: 115\n",
      "Number of 3s:  65\n",
      "Number of 6s:  72\n",
      "Number of contradictory images:  22\n",
      "Initial number of examples:  1968\n",
      "Remaining non-contradictory examples:  890\n",
      "\n",
      "Release memory.\n",
      "qnn_da_1\n",
      "Step 100: Train Loss: 0.764, Train Accuracy: 86.298%, Test Loss: 0.775, Test Accuracy: 85.393%\n",
      "Step 200: Train Loss: 0.250, Train Accuracy: 93.998%, Test Loss: 0.261, Test Accuracy: 94.270%\n",
      "Step 300: Train Loss: 0.130, Train Accuracy: 94.163%, Test Loss: 0.156, Test Accuracy: 94.494%\n",
      "Step 400: Train Loss: 0.085, Train Accuracy: 96.574%, Test Loss: 0.125, Test Accuracy: 96.180%\n",
      "Step 500: Train Loss: 0.065, Train Accuracy: 97.260%, Test Loss: 0.129, Test Accuracy: 96.292%\n",
      "Step 600: Train Loss: 0.058, Train Accuracy: 97.479%, Test Loss: 0.131, Test Accuracy: 96.292%\n",
      "Step 700: Train Loss: 0.056, Train Accuracy: 97.643%, Test Loss: 0.126, Test Accuracy: 96.404%\n",
      "Step 800: Train Loss: 0.053, Train Accuracy: 97.506%, Test Loss: 0.132, Test Accuracy: 96.404%\n",
      "Step 900: Train Loss: 0.050, Train Accuracy: 97.643%, Test Loss: 0.146, Test Accuracy: 96.404%\n",
      "Step 1000: Train Loss: 0.048, Train Accuracy: 97.643%, Test Loss: 0.148, Test Accuracy: 96.404%\n",
      "Release memory.\n",
      "qnn_da_2\n",
      "Step 100: Train Loss: 0.833, Train Accuracy: 80.378%, Test Loss: 0.826, Test Accuracy: 80.225%\n",
      "Step 200: Train Loss: 0.312, Train Accuracy: 90.025%, Test Loss: 0.349, Test Accuracy: 88.764%\n",
      "Step 300: Train Loss: 0.172, Train Accuracy: 90.381%, Test Loss: 0.234, Test Accuracy: 88.090%\n",
      "Step 400: Train Loss: 0.121, Train Accuracy: 97.698%, Test Loss: 0.210, Test Accuracy: 97.079%\n",
      "Step 500: Train Loss: 0.095, Train Accuracy: 98.438%, Test Loss: 0.189, Test Accuracy: 98.202%\n",
      "Step 600: Train Loss: 0.076, Train Accuracy: 98.849%, Test Loss: 0.189, Test Accuracy: 98.989%\n",
      "Step 700: Train Loss: 0.063, Train Accuracy: 98.822%, Test Loss: 0.199, Test Accuracy: 98.989%\n",
      "Step 800: Train Loss: 0.056, Train Accuracy: 98.904%, Test Loss: 0.218, Test Accuracy: 98.989%\n",
      "Step 900: Train Loss: 0.053, Train Accuracy: 98.904%, Test Loss: 0.234, Test Accuracy: 98.989%\n",
      "Step 1000: Train Loss: 0.050, Train Accuracy: 99.068%, Test Loss: 0.262, Test Accuracy: 99.551%\n",
      "Release memory.\n",
      "qnn_da_3\n",
      "Step 100: Train Loss: 0.656, Train Accuracy: 93.368%, Test Loss: 0.659, Test Accuracy: 93.596%\n",
      "Step 200: Train Loss: 0.206, Train Accuracy: 95.972%, Test Loss: 0.226, Test Accuracy: 95.056%\n",
      "Step 300: Train Loss: 0.094, Train Accuracy: 96.520%, Test Loss: 0.127, Test Accuracy: 95.618%\n",
      "Step 400: Train Loss: 0.072, Train Accuracy: 96.520%, Test Loss: 0.103, Test Accuracy: 95.618%\n",
      "Step 500: Train Loss: 0.069, Train Accuracy: 96.520%, Test Loss: 0.097, Test Accuracy: 95.618%\n",
      "Step 600: Train Loss: 0.068, Train Accuracy: 96.520%, Test Loss: 0.098, Test Accuracy: 95.618%\n",
      "Step 700: Train Loss: 0.068, Train Accuracy: 96.520%, Test Loss: 0.097, Test Accuracy: 95.618%\n",
      "Step 800: Train Loss: 0.067, Train Accuracy: 96.520%, Test Loss: 0.097, Test Accuracy: 95.618%\n",
      "Step 900: Train Loss: 0.067, Train Accuracy: 96.520%, Test Loss: 0.098, Test Accuracy: 95.618%\n",
      "Step 1000: Train Loss: 0.066, Train Accuracy: 96.520%, Test Loss: 0.101, Test Accuracy: 95.618%\n"
     ]
    }
   ],
   "source": [
    "x_train_tfcirc, y_train_hinge = get_mnist_train(16, a=3, b=6)\n",
    "x_test_tfcirc, y_test_hinge = get_mnist_test(16, a=3, b=6)\n",
    "\n",
    "for REP in range(1, 4):\n",
    "    try:\n",
    "        del model_circuit, model_readout, model, optimizer, loss_function\n",
    "        print('Release memory.')\n",
    "    except:\n",
    "        pass\n",
    "    finally:\n",
    "        model_name = 'qnn_da_{}'.format(REP)\n",
    "        print(model_name)\n",
    "        model_circuit, model_readout = create_quantum_model(num_qubits=16)\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(), dtype=tf.string),\n",
    "            tfq.layers.PQC(model_circuit, model_readout),\n",
    "            ], name=model_name)\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "        loss_function = tf.keras.losses.Hinge()\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(x_batch, y_batch):\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x_batch, training=True)\n",
    "                loss = loss_function(y_batch, logits)\n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        data = train_qnn(x_train_tfcirc, y_train_hinge, x_test_tfcirc, y_test_hinge, \n",
    "                         batch_size=32, num_steps=1000, intval=10)\n",
    "\n",
    "        data = np.array(data)\n",
    "        data_path = os.path.join(data_dir, 'qnn_da_{}.npy'.format(REP))\n",
    "        np.save(data_path, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xj7vwkBs59sB"
   },
   "source": [
    "# 2 vs 5 (16 Qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4819,
     "status": "ok",
     "timestamp": 1593574860125,
     "user": {
      "displayName": "Nanqing Dong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggv6drEa6aTWlDrOv1fiTXBiKpXDvz1Pm8yIE8vgg=s64",
      "userId": "15665172758075970106"
     },
     "user_tz": -60
    },
    "id": "Ih0KZNZTZVDK",
    "outputId": "542cd44b-6e9e-4a22-f3f1-5e12b91809c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "Number of unique images: 195\n",
      "Number of 2s:  143\n",
      "Number of 5s:  121\n",
      "Number of contradictory images:  69\n",
      "Initial number of examples:  11379\n",
      "Remaining non-contradictory examples:  1419\n",
      "\n",
      "Number of unique images: 107\n",
      "Number of 2s:  82\n",
      "Number of 5s:  60\n",
      "Number of contradictory images:  35\n",
      "Initial number of examples:  1924\n",
      "Remaining non-contradictory examples:  382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train_tfcirc, y_train_hinge = get_mnist_train(16, a=2, b=5)\n",
    "x_test_tfcirc, y_test_hinge = get_mnist_test(16, a=2, b=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9506808,
     "status": "ok",
     "timestamp": 1593586280220,
     "user": {
      "displayName": "Nanqing Dong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggv6drEa6aTWlDrOv1fiTXBiKpXDvz1Pm8yIE8vgg=s64",
      "userId": "15665172758075970106"
     },
     "user_tz": -60
    },
    "id": "dqH4irL-aHdI",
    "outputId": "c38f8469-6601-4944-eacc-16f1216cfef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "pqc_2 (PQC)                  (None, 1)                 32        \n",
      "=================================================================\n",
      "Total params: 32\n",
      "Trainable params: 32\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1419 samples, validate on 382 samples\n",
      "Epoch 1/30\n",
      "1419/1419 [==============================] - 318s 224ms/sample - loss: 1.0110 - hinge_accuracy: 0.2891 - val_loss: 0.9988 - val_hinge_accuracy: 0.5135\n",
      "Epoch 2/30\n",
      "1419/1419 [==============================] - 317s 224ms/sample - loss: 0.9935 - hinge_accuracy: 0.6453 - val_loss: 0.9939 - val_hinge_accuracy: 0.5783\n",
      "Epoch 3/30\n",
      "1419/1419 [==============================] - 320s 225ms/sample - loss: 0.9400 - hinge_accuracy: 0.8299 - val_loss: 0.9184 - val_hinge_accuracy: 0.7773\n",
      "Epoch 4/30\n",
      "1419/1419 [==============================] - 321s 226ms/sample - loss: 0.6949 - hinge_accuracy: 0.8778 - val_loss: 0.6342 - val_hinge_accuracy: 0.7955\n",
      "Epoch 5/30\n",
      "1419/1419 [==============================] - 321s 226ms/sample - loss: 0.4046 - hinge_accuracy: 0.9250 - val_loss: 0.4449 - val_hinge_accuracy: 0.8477\n",
      "Epoch 6/30\n",
      "1419/1419 [==============================] - 313s 221ms/sample - loss: 0.2466 - hinge_accuracy: 0.9611 - val_loss: 0.3460 - val_hinge_accuracy: 0.8741\n",
      "Epoch 7/30\n",
      "1419/1419 [==============================] - 315s 222ms/sample - loss: 0.1647 - hinge_accuracy: 0.9646 - val_loss: 0.3019 - val_hinge_accuracy: 0.8689\n",
      "Epoch 8/30\n",
      "1419/1419 [==============================] - 313s 220ms/sample - loss: 0.1271 - hinge_accuracy: 0.9611 - val_loss: 0.2836 - val_hinge_accuracy: 0.8689\n",
      "Epoch 9/30\n",
      "1419/1419 [==============================] - 313s 221ms/sample - loss: 0.1086 - hinge_accuracy: 0.9584 - val_loss: 0.2763 - val_hinge_accuracy: 0.8689\n",
      "Epoch 10/30\n",
      "1419/1419 [==============================] - 310s 219ms/sample - loss: 0.0991 - hinge_accuracy: 0.9570 - val_loss: 0.2715 - val_hinge_accuracy: 0.8689\n",
      "Epoch 11/30\n",
      "1419/1419 [==============================] - 312s 220ms/sample - loss: 0.0935 - hinge_accuracy: 0.9570 - val_loss: 0.2658 - val_hinge_accuracy: 0.8689\n",
      "Epoch 12/30\n",
      "1419/1419 [==============================] - 313s 220ms/sample - loss: 0.0903 - hinge_accuracy: 0.9570 - val_loss: 0.2643 - val_hinge_accuracy: 0.8689\n",
      "Epoch 13/30\n",
      "1419/1419 [==============================] - 313s 221ms/sample - loss: 0.0879 - hinge_accuracy: 0.9583 - val_loss: 0.2628 - val_hinge_accuracy: 0.8689\n",
      "Epoch 14/30\n",
      "1419/1419 [==============================] - 314s 221ms/sample - loss: 0.0864 - hinge_accuracy: 0.9583 - val_loss: 0.2623 - val_hinge_accuracy: 0.8689\n",
      "Epoch 15/30\n",
      "1419/1419 [==============================] - 315s 222ms/sample - loss: 0.0855 - hinge_accuracy: 0.9570 - val_loss: 0.2623 - val_hinge_accuracy: 0.8689\n",
      "Epoch 16/30\n",
      "1419/1419 [==============================] - 314s 221ms/sample - loss: 0.0850 - hinge_accuracy: 0.9583 - val_loss: 0.2615 - val_hinge_accuracy: 0.8689\n",
      "Epoch 17/30\n",
      "1419/1419 [==============================] - 328s 231ms/sample - loss: 0.0845 - hinge_accuracy: 0.9583 - val_loss: 0.2609 - val_hinge_accuracy: 0.8689\n",
      "Epoch 18/30\n",
      "1419/1419 [==============================] - 319s 225ms/sample - loss: 0.0842 - hinge_accuracy: 0.9583 - val_loss: 0.2612 - val_hinge_accuracy: 0.8689\n",
      "Epoch 19/30\n",
      "1419/1419 [==============================] - 315s 222ms/sample - loss: 0.0839 - hinge_accuracy: 0.9583 - val_loss: 0.2615 - val_hinge_accuracy: 0.8689\n",
      "Epoch 20/30\n",
      "1419/1419 [==============================] - 313s 220ms/sample - loss: 0.0838 - hinge_accuracy: 0.9583 - val_loss: 0.2612 - val_hinge_accuracy: 0.8689\n",
      "Epoch 21/30\n",
      "1419/1419 [==============================] - 314s 221ms/sample - loss: 0.0836 - hinge_accuracy: 0.9570 - val_loss: 0.2613 - val_hinge_accuracy: 0.8689\n",
      "Epoch 22/30\n",
      "1419/1419 [==============================] - 315s 222ms/sample - loss: 0.0835 - hinge_accuracy: 0.9583 - val_loss: 0.2611 - val_hinge_accuracy: 0.8689\n",
      "Epoch 23/30\n",
      "1419/1419 [==============================] - 318s 224ms/sample - loss: 0.0833 - hinge_accuracy: 0.9570 - val_loss: 0.2611 - val_hinge_accuracy: 0.8689\n",
      "Epoch 24/30\n",
      "1419/1419 [==============================] - 321s 226ms/sample - loss: 0.0832 - hinge_accuracy: 0.9583 - val_loss: 0.2612 - val_hinge_accuracy: 0.8689\n",
      "Epoch 25/30\n",
      "1419/1419 [==============================] - 319s 225ms/sample - loss: 0.0830 - hinge_accuracy: 0.9583 - val_loss: 0.2614 - val_hinge_accuracy: 0.8689\n",
      "Epoch 26/30\n",
      "1419/1419 [==============================] - 319s 225ms/sample - loss: 0.0829 - hinge_accuracy: 0.9583 - val_loss: 0.2612 - val_hinge_accuracy: 0.8689\n",
      "Epoch 27/30\n",
      "1419/1419 [==============================] - 317s 224ms/sample - loss: 0.0827 - hinge_accuracy: 0.9583 - val_loss: 0.2618 - val_hinge_accuracy: 0.8689\n",
      "Epoch 28/30\n",
      "1419/1419 [==============================] - 318s 224ms/sample - loss: 0.0825 - hinge_accuracy: 0.9583 - val_loss: 0.2614 - val_hinge_accuracy: 0.8689\n",
      "Epoch 29/30\n",
      "1419/1419 [==============================] - 326s 230ms/sample - loss: 0.0824 - hinge_accuracy: 0.9583 - val_loss: 0.2614 - val_hinge_accuracy: 0.8689\n",
      "Epoch 30/30\n",
      "1419/1419 [==============================] - 320s 225ms/sample - loss: 0.0821 - hinge_accuracy: 0.9570 - val_loss: 0.2613 - val_hinge_accuracy: 0.8689\n",
      "382/382 [==============================] - 3s 7ms/sample - loss: 0.2613 - hinge_accuracy: 0.8689\n"
     ]
    }
   ],
   "source": [
    "model_circuit, model_readout = create_quantum_model(num_qubits=16)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(), dtype=tf.string),\n",
    "    tfq.layers.PQC(model_circuit, model_readout),\n",
    "    ])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.Hinge(),\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[hinge_accuracy])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "qnn_history = model.fit(\n",
    "      x_train_tfcirc, y_train_hinge,\n",
    "      batch_size=32,\n",
    "      epochs=30,\n",
    "      verbose=1,\n",
    "      validation_data=(x_test_tfcirc, y_test_hinge))\n",
    "\n",
    "qnn_results = model.evaluate(x_test_tfcirc, y_test_hinge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1445,
     "status": "ok",
     "timestamp": 1593587541060,
     "user": {
      "displayName": "Nanqing Dong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggv6drEa6aTWlDrOv1fiTXBiKpXDvz1Pm8yIE8vgg=s64",
      "userId": "15665172758075970106"
     },
     "user_tz": -60
    },
    "id": "n-hTlRzu56qQ",
    "outputId": "a0afd844-4fa2-4d87-b692-45901dd2ae2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique images: 195\n",
      "Number of 2s:  143\n",
      "Number of 5s:  121\n",
      "Number of contradictory images:  69\n",
      "Initial number of examples:  11379\n",
      "Remaining non-contradictory examples:  1419\n",
      "\n",
      "Number of unique images: 107\n",
      "Number of 2s:  82\n",
      "Number of 5s:  60\n",
      "Number of contradictory images:  35\n",
      "Initial number of examples:  1924\n",
      "Remaining non-contradictory examples:  382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train_bin, y_train = get_mnist_train(16, a=2, b=5, quantum=False)\n",
    "x_test_bin, y_test = get_mnist_test(16, a=2, b=5, quantum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 988
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1557,
     "status": "ok",
     "timestamp": 1593587687595,
     "user": {
      "displayName": "Nanqing Dong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggv6drEa6aTWlDrOv1fiTXBiKpXDvz1Pm8yIE8vgg=s64",
      "userId": "15665172758075970106"
     },
     "user_tz": -60
    },
    "id": "YgxWQNS_-sgF",
    "outputId": "c100c4d5-098a-4a2c-9f87-e47c0b99e004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 37\n",
      "Trainable params: 37\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1419 samples, validate on 382 samples\n",
      "Epoch 1/20\n",
      "1419/1419 - 0s - loss: 0.5724 - accuracy: 0.4066 - val_loss: 0.7145 - val_accuracy: 0.4110\n",
      "Epoch 2/20\n",
      "1419/1419 - 0s - loss: 0.5580 - accuracy: 0.4306 - val_loss: 0.7160 - val_accuracy: 0.4110\n",
      "Epoch 3/20\n",
      "1419/1419 - 0s - loss: 0.5437 - accuracy: 0.4376 - val_loss: 0.7186 - val_accuracy: 0.4110\n",
      "Epoch 4/20\n",
      "1419/1419 - 0s - loss: 0.5292 - accuracy: 0.5525 - val_loss: 0.7229 - val_accuracy: 0.4110\n",
      "Epoch 5/20\n",
      "1419/1419 - 0s - loss: 0.5146 - accuracy: 0.6152 - val_loss: 0.7296 - val_accuracy: 0.3874\n",
      "Epoch 6/20\n",
      "1419/1419 - 0s - loss: 0.5002 - accuracy: 0.7315 - val_loss: 0.7402 - val_accuracy: 0.3874\n",
      "Epoch 7/20\n",
      "1419/1419 - 0s - loss: 0.4859 - accuracy: 0.7865 - val_loss: 0.7539 - val_accuracy: 0.3770\n",
      "Epoch 8/20\n",
      "1419/1419 - 0s - loss: 0.4717 - accuracy: 0.8943 - val_loss: 0.7685 - val_accuracy: 0.3770\n",
      "Epoch 9/20\n",
      "1419/1419 - 0s - loss: 0.4575 - accuracy: 0.8999 - val_loss: 0.7837 - val_accuracy: 0.3639\n",
      "Epoch 10/20\n",
      "1419/1419 - 0s - loss: 0.4434 - accuracy: 0.9091 - val_loss: 0.8012 - val_accuracy: 0.3325\n",
      "Epoch 11/20\n",
      "1419/1419 - 0s - loss: 0.4293 - accuracy: 0.9190 - val_loss: 0.8190 - val_accuracy: 0.2984\n",
      "Epoch 12/20\n",
      "1419/1419 - 0s - loss: 0.4155 - accuracy: 0.9211 - val_loss: 0.8372 - val_accuracy: 0.2880\n",
      "Epoch 13/20\n",
      "1419/1419 - 0s - loss: 0.4022 - accuracy: 0.9204 - val_loss: 0.8572 - val_accuracy: 0.2880\n",
      "Epoch 14/20\n",
      "1419/1419 - 0s - loss: 0.3892 - accuracy: 0.9260 - val_loss: 0.8783 - val_accuracy: 0.0969\n",
      "Epoch 15/20\n",
      "1419/1419 - 0s - loss: 0.3766 - accuracy: 0.9401 - val_loss: 0.8993 - val_accuracy: 0.0864\n",
      "Epoch 16/20\n",
      "1419/1419 - 0s - loss: 0.3642 - accuracy: 0.9471 - val_loss: 0.9219 - val_accuracy: 0.0733\n",
      "Epoch 17/20\n",
      "1419/1419 - 0s - loss: 0.3520 - accuracy: 0.9471 - val_loss: 0.9460 - val_accuracy: 0.0576\n",
      "Epoch 18/20\n",
      "1419/1419 - 0s - loss: 0.3401 - accuracy: 0.9471 - val_loss: 0.9702 - val_accuracy: 0.0576\n",
      "Epoch 19/20\n",
      "1419/1419 - 0s - loss: 0.3288 - accuracy: 0.9514 - val_loss: 0.9947 - val_accuracy: 0.0576\n",
      "Epoch 20/20\n",
      "1419/1419 - 0s - loss: 0.3178 - accuracy: 0.9690 - val_loss: 1.0175 - val_accuracy: 0.0576\n",
      "382/382 [==============================] - 0s 51us/sample - loss: 1.0175 - accuracy: 0.0576\n"
     ]
    }
   ],
   "source": [
    "model = create_fair_classical_model(num_qubits=16)\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(x_train_bin,\n",
    "          y_train,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          verbose=2,\n",
    "          validation_data=(x_test_bin, y_test))\n",
    "\n",
    "fair_nn_results = model.evaluate(x_test_bin, y_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP3TDVxa2fVEe2uXzu1vuCr",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Experiments.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
